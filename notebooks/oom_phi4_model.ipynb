{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-04 06:14:37 config.py:208] Replacing legacy 'type' key with 'rope_type'\n",
      "INFO 08-04 06:14:37 config.py:549] This model supports multiple tasks: {'embed', 'score', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 08-04 06:14:37 arg_utils.py:1197] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 08-04 06:14:37 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='microsoft/Phi-4-mini-instruct', speculative_config=None, tokenizer='microsoft/Phi-4-mini-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/Phi-4-mini-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 08-04 06:14:38 model_runner.py:1110] Starting to load model microsoft/Phi-4-mini-instruct...\n",
      "INFO 08-04 06:14:38 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a023a6cd294339923cdae14bc738a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-04 06:14:39 model_runner.py:1115] Loading model weights took 7.1452 GB\n",
      "INFO 08-04 06:14:44 worker.py:267] Memory profiling takes 4.26 seconds\n",
      "INFO 08-04 06:14:44 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB\n",
      "INFO 08-04 06:14:44 worker.py:267] model weights take 7.15GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 8.25GiB; the rest of the memory reserved for KV Cache is 55.84GiB.\n",
      "INFO 08-04 06:14:44 executor_base.py:111] # cuda blocks: 28588, # CPU blocks: 2048\n",
      "INFO 08-04 06:14:44 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 3.49x\n",
      "INFO 08-04 06:14:44 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-04 06:15:00 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.05 GiB\n",
      "INFO 08-04 06:15:00 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 20.52 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-04 06:15:00 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it, est. speed input: 86.21 toks/s, output: 104.10 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "To solve the equation 2x + 3 = 7, follow these steps:\n",
      "\n",
      "1. Subtract 3 from both sides of the equation to isolate the term with the variable (x) on one side:\n",
      "   2x + 3 - 3 = 7 - 3\n",
      "   2x = 4\n",
      "\n",
      "2. Divide both sides of the equation by 2 to solve for x:\n",
      "   2x / 2 = 4 / 2\n",
      "   x = 2\n",
      "\n",
      "So, the solution to the equation 2x + 3 = 7 is x = 2.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "import gc\n",
    "\n",
    "# 开启记录，并设置最多记录100000个数据点\n",
    "torch.cuda.memory._record_memory_history(max_entries=100000)\n",
    "\n",
    "llm = LLM(model=\"microsoft/Phi-4-mini-instruct\", trust_remote_code=True)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "  max_tokens=500,\n",
    "  temperature=0.0,\n",
    ")\n",
    "\n",
    "output = llm.chat(messages=messages, sampling_params=sampling_params)\n",
    "print(\"x\" * 50)\n",
    "print(output[0].outputs[0].text)\n",
    "\n",
    "# 保存数据\n",
    "torch.cuda.memory._dump_snapshot(\"dump_snapshot.pickle\")\n",
    "\n",
    "# 停掉记录，关闭snapshot\n",
    "torch.cuda.memory._record_memory_history(enabled=None)\n",
    "\n",
    "# # 释放内存\n",
    "# del llm\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "stats = torch.cuda.memory_stats(device=0)\n",
    "items = sorted(stats.items(), key=lambda kv: -kv[1])\n",
    "\n",
    "table = Table(title=\"CUDA Allocator Stats\", expand=False, show_lines=False)\n",
    "table.add_column(\"Metric\", justify=\"left\")\n",
    "table.add_column(\"Current MiB\", justify=\"right\")\n",
    "\n",
    "for k, v in items:\n",
    "    table.add_row(k, f\"{v/1024**2:,.1f}\")\n",
    "\n",
    "console = Console()\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ebc446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'free_os': 78042.5625, 'free_inside': 58.875, 'fragmented': 58.875, 'usable_by_pytorch': 0.0, 'reserved': 108.0, 'allocated': 49.125}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def torch_memory_available(device=0):\n",
    "    free_os, total_os = torch.cuda.mem_get_info(device)  # OS级别真的剩余显存（bytes）\n",
    "    res = torch.cuda.memory_reserved(device)             # caching allocator 已预留内存\n",
    "    alloc = torch.cuda.memory_allocated(device)          # tensors 占用内存\n",
    "    stats = torch.cuda.memory_stats(device)\n",
    "    frag = stats.get(\"inactive_split_bytes.all.current\", 0)\n",
    "    free_inside = res - alloc                             # allocator 内部理论还剩多少没有使用\n",
    "    can_reuse = max(free_inside - frag, 0)                # 考虑碎片化之后 PyTorch 可用部分\n",
    "    return {\n",
    "        \"free_os\": free_os,           # 其实 OS 级可用的显存（包括其他应用预留）\n",
    "        \"free_inside\": free_inside,   # PyTorch 直接复用缓存池可用大小\n",
    "        \"fragmented\": frag,           # 分配碎片造成的不可复用部分\n",
    "        \"usable_by_pytorch\": can_reuse,\n",
    "        \"reserved\": res,\n",
    "        \"allocated\": alloc,\n",
    "    }\n",
    "\n",
    "d = torch_memory_available(0)\n",
    "print({k: v / 1024**2 for k, v in d.items()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
