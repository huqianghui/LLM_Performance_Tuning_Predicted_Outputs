{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61bce3e",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/assisted-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example showcasing the impact of batched generation. Measurement device: RTX3090\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(\"cuda\")\n",
    "inputs = tokenizer([\"Hello world\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "def print_tokens_per_second(batch_size):\n",
    "    new_tokens = 100\n",
    "    cumulative_time = 0\n",
    "\n",
    "    # warmup\n",
    "    model.generate(\n",
    "        **inputs, do_sample=True, max_new_tokens=new_tokens, num_return_sequences=batch_size\n",
    "    )\n",
    "\n",
    "    for _ in range(10):\n",
    "        start = time.time()\n",
    "        model.generate(\n",
    "            **inputs, do_sample=True, max_new_tokens=new_tokens, num_return_sequences=batch_size\n",
    "        )\n",
    "        cumulative_time += time.time() - start\n",
    "    print(f\"Tokens per second: {new_tokens * batch_size * 10 / cumulative_time:.1f}\")\n",
    "\n",
    "print_tokens_per_second(1)   # Tokens per second: 418.3\n",
    "print_tokens_per_second(64)  # Tokens per second: 16266.2 (~39x more tokens per second)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ec80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "inputs = tok([\"The\"], return_tensors=\"pt\")\n",
    "generated = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n",
    "forward_confirmation = model(generated).logits.argmax(-1)\n",
    "\n",
    "# We exclude the opposing tips from each sequence: the forward pass returns\n",
    "# the logits for the next token, so it is shifted by one position.\n",
    "print(generated[0, 1:].tolist() == forward_confirmation[0, :-1].tolist())  # True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655569e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "prompt = \"Alice and Bob\"\n",
    "checkpoint = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "assistant_checkpoint = \"EleutherAI/pythia-160m-deduped\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint).to(device)\n",
    "outputs = model.generate(**inputs, assistant_model=assistant_model)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "# ['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
